{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Phrase-level Semantic Search.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPqpF3gH4A1gy1H9PQqKuKK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soniajoseph/phrase-similarity/blob/master/Phrase_Level_Semantic_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9vMX3pXFCN-",
        "colab_type": "text"
      },
      "source": [
        "## First let's load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL0uHKsWAI3P",
        "colab_type": "code",
        "outputId": "e9d2890f-3116-444a-8867-1b2aca046e21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "import urllib.request\n",
        "import re \n",
        "import numpy as np\n",
        "import numpy\n",
        "import string\n",
        "\n",
        "!python -m spacy download en_core_web_md\n",
        "import spacy\n",
        "import en_core_web_md"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4MB)\n",
            "\u001b[K     |████████████████████████████████| 95.4MB 1.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.1.0-cp36-none-any.whl size=97126237 sha256=38500ec5b897229336082b821716b893d447b59b1c4fa1feaa41d2a0b39af292\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pd6weatu/wheels/c1/2c/5f/fd7f3ec336bf97b0809c86264d2831c5dfb00fc2e239d1bb01\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZqG23gs3qTd",
        "colab_type": "text"
      },
      "source": [
        "We'll initially use Microsoft Research's Paraphrase Corpus and create a list of n paraphrase pairs. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N_GgcxtFwhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(n=5):\n",
        "  '''\n",
        "  Function to get spacy model with medium neural net with the constituency parsing extension\n",
        "  described in \"Constituency Parsing with a Self-Attentive Encoder\" (2018)\n",
        "\n",
        "  Args:\n",
        "      n (int): Number of paraphrase pairs to load.\n",
        "\n",
        "  Returns:\n",
        "      new_list: A nested list of n paraphrase pairs.\n",
        "  '''\n",
        "  target_url = 'https://raw.githubusercontent.com/wasiahmad/paraphrase_identification/master/dataset/msr-paraphrase-corpus/msr_paraphrase_data.txt'\n",
        "  i = 0; data = []\n",
        "  for sentence in urllib.request.urlopen(target_url):\n",
        "      # skip first sentence\n",
        "      if i == 0: i += 1; continue\n",
        "      sentence = sentence.decode()\n",
        "      sentence =  re.split(r'\\t+', sentence)\n",
        "      data.append(sentence[1])\n",
        "      # increment counter for number of data\n",
        "      i += 1\n",
        "      if i > n*2: break \n",
        "    # turn into nested list\n",
        "  new_list = []\n",
        "  for i in range(0, len(data)-1, 2):\n",
        "    new_list.append([data[i], data[i+1]])\n",
        "\n",
        "  print(\"Data loaded\")\n",
        "\n",
        "  return new_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57LhYJP898SJ",
        "colab_type": "code",
        "outputId": "55df7549-6237-433b-ce80-4a649168326d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data = load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uzw9_bOsFJi2",
        "colab_type": "text"
      },
      "source": [
        "## Now let's chunk the sentence into phrases.\n",
        "\n",
        "Let's load a model from spaCy..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJO3NGorxpXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model():\n",
        "  '''\n",
        "  Function to get spacy model with medium neural net with the constituency parsing extension\n",
        "  described in \"Constituency Parsing with a Self-Attentive Encoder\" (2018)\n",
        "\n",
        "  Args:\n",
        "      None\n",
        "\n",
        "  Returns:\n",
        "      nlp: A loaded model with constituency parsing functionality.\n",
        "  '''\n",
        "  nlp = en_core_web_md.load()\n",
        "  print(\"Model loaded\")\n",
        "  return nlp "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RngOUswQ9y3b",
        "colab_type": "code",
        "outputId": "5b2a46bc-ffdc-4581-c37d-1524e704870d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "nlp = get_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs3r8WfNHO0g",
        "colab_type": "text"
      },
      "source": [
        "Let's build a Traverse object to extract noun phrases from our sentences. The object will take phrases from the Stanford Parser and traverse them to find the noun phrases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36H9CSFHQoDH",
        "colab_type": "code",
        "outputId": "177a91f2-9f69-41e1-9fa2-90ff5493699e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.parse.stanford import StanfordParser\n",
        "import os\n",
        "from os import path\n",
        "\n",
        "# Create an object to recursive travel tree and collect phrases\n",
        "class Traverse():\n",
        "  '''\n",
        "  Traverse object to create trees to find noun phrases.\n",
        "  To use, call traverse_tree() with input from the StanfordParser\n",
        "  Then call the phrase_strings() function with self.phrases to get the noun phrases of\n",
        "  the input sentence.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    self.phrases = []\n",
        "    \n",
        "  def traverse_phrase(self, tree, phrases): \n",
        "      for subtree in tree:\n",
        "          if type(subtree) == nltk.tree.Tree:\n",
        "              self.traverse_phrase(subtree, phrases)\n",
        "          else:\n",
        "              phrases.append(subtree)\n",
        "\n",
        "  def traverse_tree(self, tree):\n",
        "      for subtree in tree:\n",
        "          if type(subtree) == nltk.tree.Tree:\n",
        "              if subtree.label() == 'NP' or subtree.label() == 'PP':\n",
        "                  self.traverse_phrase(subtree, self.phrases)\n",
        "                  self.phrases.append('\\n')\n",
        "              else :\n",
        "                  self.traverse_tree(subtree)\n",
        "\n",
        "  def phrase_strings(self, phrase_list):\n",
        "    # put noun phrases in list\n",
        "    a = \" \".join(phrase_list).split(\"\\n\")\n",
        "    a = [i.strip() for i in a if i]\n",
        "    return a\n",
        "\n",
        "\n",
        "# Load Stanford Statistical Parser\n",
        "def load_parser():\n",
        "  '''\n",
        "  Function to load Stanford parser,\n",
        "\n",
        "  Args:\n",
        "      None\n",
        "\n",
        "  Returns:\n",
        "      parser: return parser object\n",
        "  '''\n",
        "  path_exists = os.path.exists('/content/stanford-parser-full-2018-10-17')\n",
        "  if path_exists:\n",
        "    print(True)\n",
        "  else:\n",
        "    print(\"Load and configure StanfordParser\")\n",
        "    !wget https://nlp.stanford.edu/software/stanford-parser-full-2018-10-17.zip\n",
        "    !unzip stanford-parser-full-2018-10-17.zip\n",
        "\n",
        "  stanford_parser_dir = '/content/stanford-parser-full-2018-10-17'\n",
        "  path_to_models = stanford_parser_dir  + \"/stanford-parser-3.9.2-models.jar\"\n",
        "  path_to_jar = stanford_parser_dir  + \"/stanford-parser.jar\"\n",
        "  parser=StanfordParser(path_to_models, path_to_jar)\n",
        "  return parser\n",
        "\n",
        "load_parser()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<nltk.parse.stanford.StanfordParser at 0x7f54ee16a8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJmjx6CuZ1xF",
        "colab_type": "code",
        "outputId": "80d8f73a-70d9-4ad7-ca36-d4d05cc2566d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOnTftECUgzZ",
        "colab_type": "text"
      },
      "source": [
        "## Let's write a function to calculate the average word vector for each phrase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1or5I4JBiy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wva(string):\n",
        "    '''\n",
        "    Finds document vector through an average of each word's vector.\n",
        "\n",
        "    Args: \n",
        "      string (str): Input sentence\n",
        "\n",
        "    Returns:\n",
        "      array: Word vector average\n",
        "    '''\n",
        "    doc = nlp(string)\n",
        "    wvs = np.array([doc[i].vector for i in range(len(doc))])\n",
        "    return np.mean(wvs, axis=0) \n",
        "  \n",
        "def match_wv_pair(phrasesA, phrasesB):\n",
        "  '''\n",
        "  Takes two lists of phrases from one sentence each and finds the smallest Euclidean distance for each pair's word vector (non-exclusive).\n",
        "\n",
        "  Args:\n",
        "  phraseA (list of str): List of parsed phrases from or sentence A\n",
        "  phraseB (list of str): List of parsed phrases from sentence B to compare with sentence A \n",
        "\n",
        "  Returns:\n",
        "  matches (list of str): Returns list of matches between the two phrase (surjectively, i.e. multiple phrases can have the same match).\n",
        "\n",
        "  '''\n",
        "  # get word vectors\n",
        "  wva_a = []; wva_b = []\n",
        "  for i in phrasesA:\n",
        "    wva_a.append(wva(i))\n",
        "  for j in phrasesB:\n",
        "    wva_b.append(wva(j))\n",
        "\n",
        "  # swap so that shortest is on the outer for loop\n",
        "  if len(wva_a) > len(wva_b):\n",
        "    temp = wva_a\n",
        "    wva_a = wva_b\n",
        "    wva_b = temp\n",
        "\n",
        "    temp = phrasesA\n",
        "    phrasesA = phrasesB\n",
        "    phrasesB = temp\n",
        "\n",
        "  matches = []\n",
        "  for i in range(len(wva_a)):\n",
        "    distances = []\n",
        "    for j in range(len(wva_b)):\n",
        "      distances.append(numpy.linalg.norm(wva_a[i] - wva_b[j]))\n",
        "      # indices_total.append(np.argsort(distances)[0])\n",
        "    matches.append(\"Sentence A: \" + phrasesA[i] + \"\\n Sentence B:\" + phrasesB[np.argsort(distances)[0]] + \"\\n Euclidean Distance:\" + str(np.sort(distances)[0]) + \"\\n\")\n",
        "\n",
        "  return matches\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIcsnOb0zYh2",
        "colab_type": "text"
      },
      "source": [
        "## Now let's iterate through the phrases, calculate their word vectors, and compare the closest Euclidean distances to get matching phrases from the two sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfC_PU_tCqJj",
        "colab_type": "code",
        "outputId": "9f4b3a1d-0173-4134-f0a9-d91d7c11950a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def find_similar_phrases(data, parser):\n",
        "  '''\n",
        "  Uses Traverse object to create phrase trees of each sentence, then recurses through tree to collect noun phrases.\n",
        "\n",
        "  Args: \n",
        "  string (list of strings): List of string lists in the format [[a,b],[c,d]] to find similarity between each pair.\n",
        "\n",
        "  Returns:\n",
        "  Nothing (prints out original sentences, a phrases, b phrases, matching phrases, and their Euclidean distance)\n",
        "  '''\n",
        "  for a, b in data:\n",
        "    print(\"Original sentences:\")\n",
        "    print(\"Sentence A: \", a)\n",
        "    print(\"Sentence B: \", b)\n",
        "    print()\n",
        "\n",
        "    ta = Traverse()\n",
        "    phrasesA = parser.raw_parse(a)\n",
        "    ta.traverse_tree(phrasesA)\n",
        "    a = ta.phrases\n",
        "    a = ta.phrase_strings(a)\n",
        "    \n",
        "    tb = Traverse()\n",
        "    phrasesB = parser.raw_parse(b)\n",
        "    tb.traverse_tree(phrasesB)\n",
        "    b = tb.phrases\n",
        "    b = tb.phrase_strings(b)\n",
        "    \n",
        "    print(\"A phrases:\", a)\n",
        "    print(\"B phrases:\", b)\n",
        "    print()\n",
        "\n",
        "    matches = match_wv_pair(a,b)\n",
        "    print(\"Similar phrases:\")\n",
        "    for i in matches:\n",
        "      print(i)\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "parser = load_parser()\n",
        "find_similar_phrases(data, parser)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "Original sentences:\n",
            "Sentence A:  Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence.\n",
            "Sentence B:  Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.\n",
            "\n",
            "A phrases: ['Amrozi', \"his brother , whom he called `` the witness '' , of deliberately distorting his evidence\"]\n",
            "B phrases: [\"to him as only `` the witness ''\", 'Amrozi', 'his brother of deliberately distorting his evidence']\n",
            "\n",
            "Similar phrases:\n",
            "Sentence A: Amrozi\n",
            " Sentence B:Amrozi\n",
            " Euclidean Distance:0.0\n",
            "\n",
            "Sentence A: his brother , whom he called `` the witness '' , of deliberately distorting his evidence\n",
            " Sentence B:to him as only `` the witness ''\n",
            " Euclidean Distance:1.6207646\n",
            "\n",
            "\n",
            "\n",
            "Original sentences:\n",
            "Sentence A:  Yucaipa owned Dominick's before selling the chain to Safeway in 1998 for $2.5 billion.\n",
            "Sentence B:  Yucaipa bought Dominick's in 1995 for $693 million and sold it to Safeway for $1.8 billion in 1998.\n",
            "\n",
            "A phrases: ['Yucaipa', 'Dominick', 'before selling the chain to Safeway in 1998 for $ 2.5 billion']\n",
            "B phrases: ['Yucaipa', \"Dominick 's in 1995\", 'for $ 693 million', 'it', 'to Safeway', 'for $ 1.8 billion in 1998']\n",
            "\n",
            "Similar phrases:\n",
            "Sentence A: Yucaipa\n",
            " Sentence B:Yucaipa\n",
            " Euclidean Distance:0.0\n",
            "\n",
            "Sentence A: Dominick\n",
            " Sentence B:Dominick 's in 1995\n",
            " Euclidean Distance:5.5339174\n",
            "\n",
            "Sentence A: before selling the chain to Safeway in 1998 for $ 2.5 billion\n",
            " Sentence B:for $ 1.8 billion in 1998\n",
            " Euclidean Distance:2.0490212\n",
            "\n",
            "\n",
            "\n",
            "Original sentences:\n",
            "Sentence A:  They had published an advertisement on the Internet on June 10, offering the cargo for sale, he added.\n",
            "Sentence B:  On June 10, the ship's owners had published an advertisement on the Internet, offering the explosives for sale.\n",
            "\n",
            "A phrases: ['They', 'an advertisement on the Internet on June 10', 'the cargo for sale', 'he']\n",
            "B phrases: ['On June 10', \"the ship 's owners\", 'an advertisement on the Internet', 'the explosives', 'for sale']\n",
            "\n",
            "Similar phrases:\n",
            "Sentence A: They\n",
            " Sentence B:the ship 's owners\n",
            " Euclidean Distance:4.217091\n",
            "\n",
            "Sentence A: an advertisement on the Internet on June 10\n",
            " Sentence B:an advertisement on the Internet\n",
            " Euclidean Distance:1.383867\n",
            "\n",
            "Sentence A: the cargo for sale\n",
            " Sentence B:for sale\n",
            " Euclidean Distance:2.6837785\n",
            "\n",
            "Sentence A: he\n",
            " Sentence B:the ship 's owners\n",
            " Euclidean Distance:5.162956\n",
            "\n",
            "\n",
            "\n",
            "Original sentences:\n",
            "Sentence A:  Around 0335 GMT, Tab shares were up 19 cents, or 4.4%, at A$4.56, having earlier set a record high of A$4.57.\n",
            "Sentence B:  Tab shares jumped 20 cents, or 4.6%, to set a record closing high at A$4.57.\n",
            "\n",
            "A phrases: ['Around 0335 GMT', 'Tab shares', '19 cents , or 4.4 % ,', 'at A$ 4.56', 'a record high of A$ 4.57']\n",
            "B phrases: ['Tab shares', '20 cents , or 4.6 %', 'a record closing high', 'at A$ 4.57']\n",
            "\n",
            "Similar phrases:\n",
            "Sentence A: Tab shares\n",
            " Sentence B:Tab shares\n",
            " Euclidean Distance:0.0\n",
            "\n",
            "Sentence A: 20 cents , or 4.6 %\n",
            " Sentence B:19 cents , or 4.4 % ,\n",
            " Euclidean Distance:0.89350325\n",
            "\n",
            "Sentence A: a record closing high\n",
            " Sentence B:a record high of A$ 4.57\n",
            " Euclidean Distance:2.0649736\n",
            "\n",
            "Sentence A: at A$ 4.57\n",
            " Sentence B:at A$ 4.56\n",
            " Euclidean Distance:0.0\n",
            "\n",
            "\n",
            "\n",
            "Original sentences:\n",
            "Sentence A:  The stock rose $2.11, or about 11 percent, to close Friday at $21.51 on the New York Stock Exchange.\n",
            "Sentence B:  PG&E Corp. shares jumped $1.63 or 8 percent to $21.03 on the New York Stock Exchange on Friday.\n",
            "\n",
            "A phrases: ['The stock', '$ 2.11 , or about 11 percent', 'Friday', 'at $ 21.51 on the New York Stock Exchange']\n",
            "B phrases: ['PG&E Corp. shares', '$ 1.63 or 8 percent', 'to $ 21.03 on the New York Stock Exchange', 'on Friday']\n",
            "\n",
            "Similar phrases:\n",
            "Sentence A: The stock\n",
            " Sentence B:to $ 21.03 on the New York Stock Exchange\n",
            " Euclidean Distance:2.9479926\n",
            "\n",
            "Sentence A: $ 2.11 , or about 11 percent\n",
            " Sentence B:$ 1.63 or 8 percent\n",
            " Euclidean Distance:2.002041\n",
            "\n",
            "Sentence A: Friday\n",
            " Sentence B:on Friday\n",
            " Euclidean Distance:3.5353792\n",
            "\n",
            "Sentence A: at $ 21.51 on the New York Stock Exchange\n",
            " Sentence B:to $ 21.03 on the New York Stock Exchange\n",
            " Euclidean Distance:1.236261\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMXSiXooIGp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = \"Charlie Chan is off the case for the Fox Movie Channel.\"\n",
        "b = \"The Fox Movie Channel has banned Charlie Chan.\"\n",
        "c = \"Feelings about current business conditions improved substantially from the first quarter, jumping from 40 to 55.\"\n",
        "d = \"Assessment of current business conditions improved substantially, the Conference Board said, jumping to 55 from 40 in the first quarter.\"\n",
        "\n",
        "samples = [[a,b], [c,d]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJf5X25_IXAl",
        "colab_type": "code",
        "outputId": "dae17e2a-54d5-4e2f-ebe9-967e05532306",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "\n",
        "find_similar_phrases(samples, parser)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original sentences:\n",
            "Sentence A:  Charlie Chan is off the case for the Fox Movie Channel.\n",
            "Sentence B:  The Fox Movie Channel has banned Charlie Chan.\n",
            "\n",
            "A phrases: ['Charlie Chan', 'the case for the Fox Movie Channel']\n",
            "B phrases: ['The Fox Movie Channel', 'Charlie Chan']\n",
            "\n",
            "Similar phrases:\n",
            "Sentence A: Charlie Chan\n",
            " Sentence B:Charlie Chan\n",
            " Euclidean Distance:0.0\n",
            "\n",
            "Sentence A: the case for the Fox Movie Channel\n",
            " Sentence B:The Fox Movie Channel\n",
            " Euclidean Distance:1.7385955\n",
            "\n",
            "\n",
            "\n",
            "Original sentences:\n",
            "Sentence A:  Feelings about current business conditions improved substantially from the first quarter, jumping from 40 to 55.\n",
            "Sentence B:  Assessment of current business conditions improved substantially, the Conference Board said, jumping to 55 from 40 in the first quarter.\n",
            "\n",
            "A phrases: ['Feelings about current business conditions', 'from the first quarter', 'from 40 to 55']\n",
            "B phrases: ['Assessment of current business conditions', 'the Conference Board', 'to 55', 'from 40 in the first quarter']\n",
            "\n",
            "Similar phrases:\n",
            "Sentence A: Feelings about current business conditions\n",
            " Sentence B:Assessment of current business conditions\n",
            " Euclidean Distance:2.0531306\n",
            "\n",
            "Sentence A: from the first quarter\n",
            " Sentence B:from 40 in the first quarter\n",
            " Euclidean Distance:1.2342855\n",
            "\n",
            "Sentence A: from 40 to 55\n",
            " Sentence B:to 55\n",
            " Euclidean Distance:1.4019067\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJMtcURwnKi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}